% SPDX-FileCopyrightText: Copyright (c) 2021-2025 Yegor Bugayenko
% SPDX-License-Identifier: MIT

\documentclass[sigplan,nonacm,review,anonymous]{acmart}
\settopmatter{printfolios=false,printccs=false,printacmref=false}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{iexec} % for \iexec command
\usepackage{href-ul} % for \href command
\usepackage[noframes]{ffcode} % for \ff command
\usepackage{doi} % for better render of DOI
\usepackage{bibcop}
\usepackage{paralist}
\usepackage{ffcode}
\usepackage[capitalize]{cleveref}
\usepackage{amsmath}    % for mathematical symbols
\usepackage{graphicx}   % for advanced table formatting
\usepackage{array}      % for better table control
\usepackage{longtable}      % for better table control

\usepackage{silence}
  \WarningFilter{microtype}{Unable to apply patch `footnote'}

\title{CAM: A Collection of Snapshots of GitHub Java Repositories Together with Metrics}
\author{Yegor Bugayenko}
\orcid{0000-0001-6370-0678}
\email{yegor256@huawei.com}
\affiliation{\institution{Huawei, Russia, Moscow}\city{}\country{}}

\makeatletter
\let\@authorsaddresses\@empty
\makeatother

\tolerance=2000

\newcommand\cam{{\sffamily\bfseries CAM}}

\begin{document}

% check this paper, it looks similar to what we are doing: https://ieeexplore.ieee.org/abstract/document/8816794

\begin{abstract}
Even though numerous researchers require stable datasets along with source code
and basic metrics calculated on them, neither GitHub nor any other code hosting
platform provides such a resource. Consequently, each researcher must download
their own data, compute the necessary metrics, and then publish the dataset
somewhere to ensure it remains accessible indefinitely. Our \cam{} (stands for
``Classes and Metrics'') project addresses this need. It is an open-source
software capable of cloning Java repositories from GitHub, filtering out
unnecessary files, parsing Java classes, and computing metrics such as
Cyclomatic Complexity, Halstead Effort and Volume, C\&K metrics,
Maintainability Metrics, LCOM5 and HND, as well as some Git-based Metrics. At
least once a year, we execute the entire script, a process which requires a
minimum of ten days on a very powerful server, to generate a new dataset.
Subsequently, we publish it on Amazon S3, thereby ensuring its availability as
a reference for researchers. The latest archive of 2.2Gb that we published on
the 2nd of March, 2024 includes 532K Java classes with 48 metrics for each
class.
\end{abstract}

\maketitle

\section{Motivation}\label{sec:motivation}

First, research projects that analyze Java code usually extract it from
repositories where open-source projects store their files, such as GitHub. It
is common practice in papers explaining results to fully disclose the
coordinates of the open-source code being extracted. However, source code is
inherently volatile: repositories change their locations and files are
modified, as demonstrated by \citet{5463348}. To ensure the replicability of
their research results, paper authors must somehow guarantee that the source
code used at the time of research remains available and intact throughout the
paper's lifetime. One obvious solution would be to make copies of the
repositories being extracted and then host them somewhere they are ``forever''
available.

Second, research methods typically involve filtering out certain types of files
found in repositories, such as plain text documents or graphic images, which
are not source code. Additionally, some source code files may need to be
excluded because they are auto-generated or contain unparsable Java code,
making them unsuitable for most methods of code analysis.

Third, most source code analysis research involves collecting metrics from the
files found in extracted repositories, such as lines of code, complexity,
cohesion, and so on. Most of these metrics are already known, and their
retrieval mechanisms are trivial, as summarized by \citet{nunez2017source}.

Thus, there is an obvious duplication of work among different research projects:
\begin{inparaenum}[(a)]
\item they have to ``host'' extracted data to ensure desirable replicability, as noted by \citet{7887704},
\item they must implement filtering of source code fetched from GitHub, and
\item they have to collect popular metrics.
\end{inparaenum}
Having a ready-to-use archive of downloaded, filtered, and measured source code files would help many research projects reduce the amount of work required.

This article is structured as follows:
\cref{sec:method} outlines practical steps taken to build the dataset,
\cref{sec:results} describes obtained results,
\cref{sec:limitations} explores limitations,
\cref{sec:discussion} discusses some doubtful aspects of the research,
and
\cref{sec:conclusion} offers a summary of the paper.

\section{Methodology}\label{sec:method}

In order to help research projects in all three tasks mentioned above, we
created \cam{}\footnote{\url{https://github.com/yegor256/cam}} archive: an
open-source collection of scripts regularly (at least once a year) being
executed in Docker containers in our proprietary computing environment with
results published in form of an ``immutable'' ZIP archive as either a GitHub
``asset'' attached to the next release of our GitHub repository or an object in
Amazon S3 (depending on the size of the archive). Here, immutability is not
technically guaranteed but promised: even though we, being the owners of the
repository, are able to replace any previously created assets, we are not going
to do so in order to not jeopardize the idea. Instead, new releases will be
published retaining previously generated assets unmodified.

At the time of writing, our GitHub repository consists of scripts written in Makefile,
Python, Ruby, and Bash, which do exactly the following:
\begin{itemize}
    \item Fetch open repositories from GitHub, which have \ff{java} language
    tag, have reasonably big but not too big number of stars, and are of certain minimum size;
    \item Filter out repositories that have license different from MIT or Apache License.
    \item Filter out repositories those contain samples, instead real project,
    framework or library by using \ff{samples-filter}\footnote{\url{https://github.com/h1alexbel/samples-filter}}
    that predicts using text classification to which class (real or sample)
    repository belongs to.
    \item Remove files without \ff{.java} extension, Java files with syntax errors,
    supplementary files such as \ff{package-info.java} and \ff{module-info.java},
    files with very long lines, and unit tests;
    \item Calculate KLoC, NCSS, Cyclomatic Complexity~\citep{mccabe1976complexity},
    Cognitive Complexity~\citep{campbell2018cognitive}, LCOM5~\citep{henderson1996coupling},
    NHD~\citep{counsell2006interpretation}, TCC~\citep{bieman1995cohesion},
    number of attributes, number of constructors, number of methods, number of static methods,
    and some other metrics.
\end{itemize}

Then, the entire dataset is packaged as a ZIP archive and published
on GitHub.

We believe that our method is ethical, as it utilizes data from publicly
available sources, thereby avoiding any infringement of copyright.

\section{Results}\label{sec:results}

In total, \iexec{tail -n +2 "${TARGET}/repositories.csv" | wc -l}\unskip{} repositories
were found and retrieved from GitHub.
\iexec{cat "${TARGET}/temp/repo-details.tex"}
The full list of them is in the \ff{repositories.csv} file.
The \ff{hashes.csv} file has a list of Git hashes of their latest commits.
Predictions about each repository being sample or not located in \ff{predictions.csv} file.

The filtering process was the following:

\begin{enumerate}
\iexec{cat "${TARGET}"/temp/reports/*.tex}\unskip{}
\end{enumerate}

The structure of the archive is the following:

\begin{itemize}

% printf doesn't work here
  \item \ff{data/} is the collection of \ff{.csv} files with the data
  (\iexec{find "${TARGET}/data" -type f -name '*.csv' | wc -l}\unskip{} total);

  \item \ff{github/} is the entire collection of
  \iexec{find "${TARGET}/github" -type f -name '*.java' | wc -l}\unskip{}
  source files ``as is''
  (\iexec{du -sh "${TARGET}/github" | cut -f1}\unskip{} total).

\end{itemize}

Some repositories require compilation prior to analysis.
Below is a summary of the build systems used and the status of their builds:
\begin{itemize}
  \item Gradlew: successful builds: \iexec{grep -E 'Success.*Gradlew' "${TARGET}/temp/jpeek_success.log" | wc -l},
  failed builds: \iexec{grep -E 'Failure.*Gradlew' "${TARGET}/temp/jpeek_failure.log" | wc -l}.
  \item Gradle: successful builds: \iexec{grep -E 'Success.*Gradle' "${TARGET}/temp/jpeek_success.log" | wc -l},
  failed builds: \iexec{grep -E 'Failure.*Gradle' "${TARGET}/temp/jpeek_failure.log" | wc -l}.
  \item Maven: successful builds: \iexec{grep -E 'Success.*Maven' "${TARGET}/temp/jpeek_success.log" | wc -l},
  failed builds: \iexec{grep -E 'Failure.*Maven' "${TARGET}/temp/jpeek_failure.log" | wc -l}.
  \item Other: successful builds: \iexec{grep -E 'Success.*Non-build' "${TARGET}/temp/jpeek_success.log" | wc -l},
  failed builds: \iexec{grep -E 'Failure.*Non-build' "${TARGET}/temp/jpeek_failure.log" | wc -l}.
\end{itemize}

The following
\iexec{cat "${TARGET}/temp/list-of-metrics.tex" | wc -l}\unskip{}
metrics were
calculated for each \ff{.java} file:

\begin{itemize}
  \input{$TARGET/temp/list-of-metrics.tex}
\end{itemize}
Here is a graph of metrics and their aggregated values:
\iexec{cat "${TARGET}/temp/aggregation_table.tex" }\unskip{}


The dataset was built by
\iexec{nproc}\unskip{}
CPUs\iexec{"${LOCAL}/help/tdiff.sh" "$(cat "${TARGET}/start.txt")"}\unskip{}.


\section{Limitations}\label{sec:limitations}

As of January 2023, \citet{dohmke2023} reported that GitHub hosts more than
420 million repositories, including at least 28 million public repositories,
which is the world's largest source code host as of June 2023. According
to \citep{daigle2023}, Java is the 4th most popular language on GitHub. Thus,
it is reasonable to assume that there are millions of Java repositories on
GitHub. It is technically impossible to download and parse even a few percent
of this huge data source. In the \cam{} project, we download and scan only a
thousand repositories (planning to download a few thousand in the future).
Such a tiny fraction of the entire possible scope of analysis is obviously
not representative enough. Researchers must understand this limitation and
only use \cam{} when representability of the entire Java domain is not the
goal of the research.

Even though most of the metrics that we collect have formal definitions
given in the papers where the metrics were originally introduced,
for example NHD~\citep{counsell2006interpretation} and
TCC~\citep{bieman1995cohesion}, there are certain modifications
that we had to make to their original algorithms. This happened mostly
because modern Java classes have certain features that were not present
when said metrics were introduced. Researchers must understand that
the metrics generated by the scripts in \cam{} are not exactly the same
metrics that were described by their authors.

Even though our scripts download only reasonably popular Java repositories,
some of them contain Java files with broken syntax. Also, some files use
new Java syntax introduced only in recent versions of Java (such as,
for example, ``records'' introduced in Java~21).
The parser\footnote{\url{https://github.com/c2nes/javalang}} that we use
in \cam{} is only capable of parsing Java~8. We simply exclude all files
that are not parseable by this parser. Researchers who are looking for
the most current syntax of Java must remember this limitation and try
to find another source of data.

\section{Discussion}\label{sec:discussion}

\textbf{Why the number of GitHub stars was used as selection criteria?}
Obviously, the selection criteria was not perfect, for example because,
as it was demonstrated by \citet{munaiah2017curating}, the number of stars
may not always be a proxy for project quality or relevance. However,
it is the best possible indicator of popularity of a repository in GitHub.

\section{Conclusion}\label{sec:conclusion}

In this research, we downloaded Java source code from
\iexec{tail -n +2 "${TARGET}/repositories.csv" | wc -l}\unskip{}
open GitHub repositories, removed the noise, and ended up with
\iexec{find "${TARGET}/github" -type f -name '*.java' | wc -l}\unskip{} Java files.
Then, we calculated
\iexec{cat "${TARGET}/temp/list-of-metrics.tex" | wc -l}\unskip{}
for each Java file and created a ZIP archive.
We expect \cam{} archives to be used by research teams analyzing Java source, which want
\begin{inparaenum}[(a)]
\item to guarantee replicability of their results
and
\item to reduce data pre-processing efforts.
\end{inparaenum}
We also expect open-source community to contribute to \cam{} scripts,
making filtering more powerful and adding more code metrics to the collection.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography/main}

\end{document}
